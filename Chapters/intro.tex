\chapter{Introduction} 
	
	Autonomous Robots are used in more and more applications everyday. Mobile robots with autonomous navigation capability find applications in search and rescue, space exploration, and even domestic use\cite{search1,search2,marsRover}. For this thesis, an Integrated Mobile Platform (IMP) is designed as a ground vehicle capable of autonomous navigation. In this chapter, the basic concept of Simultaneous Localization and Mapping (SLAM) and its importance in autonomous navigation is discussed as well as the various methods of sensing the environment.
	
	\section{Simultaneous Localization and Mapping}
	
	In most implementations of autonomous navigation, a common challenge faced is the need for the robot to have a map of the operating environment it is in and to precisely know its own location within this map. If an accurate map of the environment is available a priori, estimating the path of the robot would be a simple localization task \cite{Thrun1999}. Similarly if the true path of the robot was known, building a map of the environment is straightforward \cite{Tripathi2014,lakemeyer2003}. However, when both the environmental map and the true path of the robot are unknown, the problem becomes challenging.
	
	A common situation where such a problem arises is when a robot is required to autonomously navigate a previously unexplored environment, or navigate in a changing environment. Examples of  applications involving such unexplored environments include space, underwater and underground explorations\cite{marsRover,mineMapping,underwaterSLAM}. Robots capable of autonomously navigating changing environments find application in both domestic and industrial scenarios where there are humans involved in the environment. The movement of humans themselves as well as the changes they make to the environment create challenges, in which a robot's ability to map the surroundings and localize itself in that map becomes invaluable. 
	
	Simultaneous Localization and Mapping addresses the problem of a robot estimating its own position in an unknown environment, by simultaneously building a map of the environment while it achieving localization. To achieve SLAM, the robot utilizes its proprioceptive sensors to get an estimate of its current state (e.g.,\ position, velocity), and then observe the environment via its exteroceptive sensors. It is understood that all sensor measurements are corrupted by noise. The measurements of the environment is then used to improve upon the estimated state while simultaneously improving the robot's map of the environment. 
	
	The two processes of localization and mapping need to be done simultaneously primarily due to the relationship between the localization error and mapping error being cyclic in nature. An increase in error in either one, will result in a larger increase in the other. 
	
	 Most SLAM implementations use odometry to calculate the initial estimate of the robot pose. This pose estimate is refined by using measurements of the environment made by other sensors. The development in the robotic community has lately shifted towards the	use of sensors that are cheaper, lighter, smaller and thus often less accurate. This change allows for use of SLAM on smaller weight-constrained robots such as Unmanned Aerial Vehicles (UAV) to a larger extent\cite{steder2008} and also increases the potential of having SLAM based consumer products, e.g.\, autonomous vacuum cleaners, on the market. Another trend in robotics is the change in the maps that represent the environment. Most early papers on SLAM used a 2 dimensional representation of the environment. While the extension of these algorithms to full three-dimensional representation has been mathematically straightforward, it is only recently, more algorithms capable of the additional computational complexity were introduced\cite{3d1,3d2,3d3}. The maps generated by SLAM can either be used on their own\cite{mineMapping,Arbeiter2010}, or as an input to path planning algorithms\cite{pp1,pp2,pp3,pp4,pp5,pp6}.  
	
	The numerous SLAM algorithms can be broadly divided into probabilistic approaches and scan-matching methods. The probabilistic approaches estimate a probability distribution that describes the joint posterior density of both the robot pose and the map. This probability distribution can be estimated by an Extended Kalman Filter (EKF) \cite{ekfSLAM1,ekfSLAM2,ekfSLAM3,ekfSLAM4}, an Unscented Kalman Filter (UKF)\cite{ukf1,ukf2,ukf3,ukf4}, or by Rao-Blackwellized Particle Filters (RBPF)\cite{particleSLAM1,particleSLAM2,particleSLAM3,particleSLAM4,particleSLAM5}. The scan-matching methods build a map by merging consecutive scans of the environment. Optimization algorithms are used to optimally align two consecutive scans. The pose increment of the robot is found by calculating the rotation and the translation that were applied to the two scans in order to align them. Scan-matching SLAM algorithms all have the Iterative Closest Point (ICP) algorithm at their core \cite{icp1}. ICP is an algorithm that minimizes the difference between two clouds of points. In the algorithm, one point cloud, the reference or target, is kept fixed while the other one, the source, is transformed to best match the reference. The algorithm iteratively revises the transformation (combination of translation and rotation) needed to minimize the distance from the source to the reference point cloud. The inverse of this transformation gives an estimate of the relative pose of the robot. 
		
	The main advantage of ICP over the other (probabilistic) SLAM algorithms is the relatively high computational speed and the ease of implementation. However, it does not provide any information on the likelihood of the estimated pose or the map. The disadvantage of using the Extended Kalman Filter for SLAM is that linearized versions of the motion and observation models are used, which can cause estimation errors. Neither the UKF nor the RBPF have this problem. Compared to the UKF, the RBPF is better at handling non-Gaussian noise. In spite of this, Extended Kalman Filter is still used in a large number of applications due to its simplicity and lower computational complexity.

	\section{Environment Sensing}	
	
	As seen in the last section, the two major components of SLAM are proprioceptive sensing for estimation of the pose of the robot and exteroceptive sensing for sensing the environment around the robot and to create a representation of the environment that the robot can understand. While the former is a function of the motion model of the robot and the odometry of the robot, the latter depends mainly on the sensor used. A large number of exteroceptive sensors can be used to gather data about the environment based on the type of robot and the type of environment the robot is in. Commonly used sensors include laser scanners\cite{lidar1, lidar2, lidar3, lidar4, lidar5}, monocular cameras \cite{mono1, mono2, mono3, mono4, mono5, mono6, mono7}, stereo vision systems \cite{stereo1, stereo2, stereo3,stereo4, stereo5} and 3D cameras such as Microsoft's Kinect\cite{kinect1,kinect2,kinect3}. Some environments such as underwater or underground require more specialized sensors such as SONAR or RADAR\cite{radar, sonar1, sonar2, sonar3}. It is also common to use multiple sensors and fuse their data to obtain a better representation of the environment. In \cite{combo1}, data from a SONAR is combined with a single camera image in SLAM. Development of such sensor fusion algorithms is an active area of research by itself \cite{combo2,combo3,combo4}.
	
	The two-dimensional scanning laser range finder (LIDAR),which measures the distance to objects in a plane, is a commonly used sensor for mapping the environment. There are two basic approaches to mapping with LIDARs, feature extraction and scan matching. The feature extraction approach looks for specific features that are predicted to be in the environment. These features are termed landmarks, and are used for localization of the robot. The predicted features that this approach looks for is a function of the environment. In indoor settings, lines, corners, and curves are known to be good features to expect\cite{indoor1, indoor2, indoor3, indoor4}, whereas in outdoor environments, tree detectors\cite{trees} have been extensively used\cite{outdoor1, outdoor2, outdoor3}. One of the disadvantages of feature based landmark detection is the lack of a general-purpose feature detector that works well in varied environments. 
	
	The alternative LIDAR approach, scan matching, directly matches point clouds. This approach dispenses entirely with features and leads to map constraints that directly relate two poses. Scan matching systems are much more adaptable as their performance is not dependent on the world containing straight lines, corners, or trees. However, scan matching has a major disadvantage, it tends to create dense pose graphs that significantly increase the computational cost of computing a posterior map. 
	
	The computational cost of feature based algorithms are lower than the cost of scan matching algorithms as searching over data associations is computationally less expensive than searching over the space of rigid body transformations. In scan matching algorithms, the computational cost at each iteration, is dependent on the error existing at the previous iteration. Hence, a large initialization error will result in large computational cost. Whereas, the cost of feature based matching is nearly independent of initialization error. Therefore, feature based algorithms are preferred over scan matching for applications within a homogeneous environment.
	
	\section {Thesis Outline}
	
	In the previous sections, it is  described that \slam  is an essential component of autonomous navigation in unknown or dynamic environments. In Chapter~\ref{cha:Overview}, the general concepts behind probabilistic SLAM is discussed. As an example of probabilistic SLAM techniques, a Gaussian filter, the Extended Kalman Filter, is developed along with its mathematical formulation. The \ekf is then presented as a solution to the SLAM problem. 
	
	Chapter~\ref{cha:Platform } details the design process of the Integrated Mobile Platform along with the motivation and implementation of the various hardware and control components. The software design for the current configuration of the platform is discussed followed by the motion model of the robot and how it can be used for SLAM. 
	
	Using the data from the LIDAR mounted on the platform described in Chapter~\ref{cha:Platform }, two different kinds of features are extracted. A simplistic algorithm to extract point features is discussed in Chapter~\ref{cha:featureExtractor} along with its drawbacks,thereafter modifications to overcome the drawbacks are suggested. Linear features such as walls are extracted first using RANSAC\cite{Fischler1981} and later using Hough transform\cite{Hu1998}. The extracted features are utilized in the SLAM algorithm detailed in Chapter~\ref{cha:Overview} and the results are presented in Chapter~\ref{cha:results}.